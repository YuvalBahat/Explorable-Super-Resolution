{
  "name": "Z1_patchDhinge1_nl6_GDratio1_AdvOnly" // Model name (a folder with this name will be created if resume=0).
  , "use_tb_logger": false
  , "model":"srragan"
  , "scale": 4
  , "gpu_ids": [0]
  , "range": [0,1]

  , "datasets": {
//    "path": "path_to_datasets_root_dir",
    "train": {
      "name": "DIV2K"
      , "mode": "LRHR"
      , "dataroot_HR": "DIV2K_train/DIV2K_train_sub_HRx4.lmdb"
      , "dataroot_LR": "DIV2K_train/DIV2K_train_sub_bicLRx4.lmdb"
      , "subset_file": null
      , "use_shuffle": true
      , "n_workers": 2 //2
      , "batch_size_per_GPU": 8
      , "batch_size_4_grads_G": 16
      , "batch_size_4_grads_D": 16
      , "patch_size": 256
      , "use_flip": true
      , "use_rot": true
    }
    , "val": {
      "name": "val_set14_part"
      , "mode": "LRHR"
      , "dataroot_HR": "Set14/Set14_HRx4" //"DIV2K_valid/DIV2K_valid_HRx4"//"Set14"
      , "dataroot_LR": "Set14/Set14_bicLRx4" //,"DIV2K_valid/DIV2K_valid_bicLRx4"//,"Set14_bicLRx4"
    }
  }

  , "path": {
    "root": "/media/ybahat/data/projects/SRGAN"
//    , "pretrained_model_G": "../pretrained_models/RRDB_ESRGAN_x4.pth" // Path to weights initialization model of the same architecture (but the Z input). For example, ESRGAN
}

  , "network_G": {
    "which_model_G": "RRDB_net"
    , "CEM_arch": 1 //Wrap architecture with CEM for consistency
    , "sigmoid_range_limit": 0
    , "latent_input": "first_layer" // Concatenate Z signal to which model layers: "all_layers" /  "first_layer" / "None"
    , "latent_input_domain": "HR_downscaled" // Spatial dimenssions of Z correspond to "HR_downscaled" / "HR_rearranged","LR"
    , "latent_channels": "SVDinNormedOut_structure_tensor" // What kind of L_struct loss term is used: ""SVDinNormedOut_structure_tensor"" / "STD_1dir" / "STD_directional" / "structure_tensor" / "SVD_structure_tensor"
    , "norm_type": null
    , "mode": "CNA"
    , "nf": 64
    , "nb": 23
    , "in_nc": 3
    , "out_nc": 3
    , "gc": 32
    , "group": 1
  }
  , "network_D": {
    "which_model_D": "PatchGAN" //,"discriminator_vgg_128"
    , "relativistic": 0 // Optionally use a relativistic discriminator (like in ESRGAN), which can be seen as s full-reference loss term (a disadvantage)
    , "decomposed_input": 0 // Feed D with low-frequencies and high-frequencies separately (unsupported)
    , "pre_clipping": 0 // Clip D's input to valid pixels range
    , "add_quantization_noise": 0 // Add noise to D's input
    , "norm_type": "batch"
    , "act_type": "leakyrelu"
    , "mode": "CNA"
    , "n_layers": 6
    , "nf": 64
    , "in_nc": 3
  }

  , "train": {
    "resume":0 // Set to 1 if resuming the training of a model with the same name.
    ,"lr_G": 1e-5 //1e-5
    , "weight_decay_G": 0
    , "beta1_G": 0.5 //0.9
    , "beta2_G": 0.9 //None
    , "lr_D": 1e-5 //1e-5
    , "weight_decay_D": 0
    , "beta1_D": 0.5 //0.9 //0.0
    , "beta2_D": 0.9 //None
    , "lr_scheme": "MultiStepLR"
    , "steps_4_loss_std": 1 //500
    , "std_4_lr_drop": 1e6 //1.6
    , "lr_change_ratio": 4 // ratio between STD and slope of linear fit, under which lr is reduced
    , "lr_steps": [50000, 100000, 200000, 300000] //[5000, 10000, 20000, 30000] //
    , "lr_gamma": 0.5
    , "pixel_domain": "HR"
    , "pixel_criterion": "l1"
    , "feature_domain": "HR"
    , "feature_criterion": "l1"
    , "gan_type": "wgan-sn" //,"vanilla","lsgan","wgan-gp"
    , "hinge_threshold": 1 //Comment if not using hinge loss
    , "optimalZ_loss_type": "l1"  //"None","hist","l1"
//    , "D_verification": "past" //'current', 'convergence', 'past'
    , "steps_4_D_convergence": 2000
    , "min_D_prob_ratio_4_G": 1.05 //Average D prob ratio should be at least this on all preceeding D_valid_Steps_4_G_update steps
    , "min_mean_D_correct": 0.9 //At least this portion of images should be correctly classified by D in all preceeding D_valid_Steps_4_G_update steps.
    , "D_update_ratio": 1 //[[4,-10],[0,0.1]] //10 //0
    , "D_valid_Steps_4_G_update": 10 //Perform G steps using l_gan only when D was successfull in previous D_update_ratio steps
    , "CEM_exp": 1 //Means I use loss mask and padding while training and testing, no matter if actually using the CEM or not.

//    , "pixel_weight": 1e-2
//    , "feature_weight": 0 //1
    , "gan_weight": 1 // Weight for adversarial loss term L_adv
//    , "latent_weight": 0 //1 //Weight on L_struct loss term, between G's output and the desired criterion given Z.
//    , "optimalZ_loss_weight": 0 // L_map loss term weight
//    , "range_weight": 5000 // L_range loss term weight

     , "D_init_iters": 0
     , "gp_weight": 0 //10

    , "manual_seed": 0
    , "niter": 510000
    , "val_freq": 500
    , "val_save_freq": 1e4
    , "val_running_avg_steps": 5
}

  , "logger": {
    "print_freq": 100 //200
    , "save_checkpoint_freq": 20 // Minutes
  }
}
